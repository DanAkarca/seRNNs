{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHpRPtFUEpUS"
   },
   "source": [
    "# seRNN Demo: How to spatially-embed an RNN\n",
    "\n",
    "In this notebook we provide a run through of the basic training routine behind spatially-embedded recurrent neural networks (seRNNs) and apply structural metrics to an example seRNN. We hope that giving researchers early access to these tools will make our implementation more understandable and allow researchers to adapt the model to their needs before full publication of the project. We will add the additional analysis script for generative models, decoding, structure-function clustering, mixed selectivity and energy-usage during the coming weeks.\n",
    "\n",
    "\n",
    "For more resources see the projects website: https://www.jachterberg.com/seRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QmtNrL09EJE6"
   },
   "outputs": [],
   "source": [
    "#pip install -q 'tensorflow==2.3.0'\n",
    "#pip install -q 'numpy==1.18.5'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KZPteGBRLGiR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.python.keras import backend\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l2RihmF-MRm1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(18229)\n",
    "tf.random.set_seed(94892)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0renWOfsIoFz"
   },
   "source": [
    "## Task / Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYqJ7U8nJIcp"
   },
   "source": [
    "### Dataset generator\n",
    "\n",
    "\n",
    "The function below generates multiple datasets representing our maze-like task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f4rZn22NI5U9"
   },
   "outputs": [],
   "source": [
    "class mazeGeneratorI():\n",
    "    '''\n",
    "    Objects of the mazeGeneratorI class can create numpy and tf datasets of the first choice of the maze task.\n",
    "    Task structure:\n",
    "        Goal presentation, followed by delay period, followed by choice options.\n",
    "    Response:\n",
    "        One response required from agent at end of episode. Direction (Left, Up, Right, Down) of first step.\n",
    "    Encoding:\n",
    "        Both observations and labels are OneHot encoded.\n",
    "    Usage:\n",
    "        The two only function a user should need to access are \"construct_numpy_data\" and \"construct_tf_data\"\n",
    "    Options:\n",
    "        Both data construction methods have an option to shuffle the labels of data.\n",
    "        The numpy data construction method allows to also return the maze identifiers.\n",
    "    '''\n",
    "    def __init__(self, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
    "        self.version = 'v1.2.0'\n",
    "        \n",
    "        # Import variables defining episode\n",
    "        self.goal_presentation_steps = goal_presentation_steps\n",
    "        self.delay_steps = delay_steps\n",
    "        self.choices_presentation_steps = choices_presentation_steps\n",
    "\n",
    "        # Construct mazes dataframe\n",
    "        ## Add encoded versions of the goal / choices presentations and the next step response\n",
    "        self.mazesdf = self.import_maze_dic()\n",
    "        self.mazesdf['Goal_Presentation'] = self.mazesdf['goal'].map({\n",
    "            7:np.concatenate((np.array([1,0,0,0]),np.repeat(0,4))),\n",
    "            9:np.concatenate((np.array([0,1,0,0]),np.repeat(0,4))),\n",
    "            17:np.concatenate((np.array([0,0,1,0]),np.repeat(0,4))),\n",
    "            19:np.concatenate((np.array([0,0,0,1]),np.repeat(0,4)))})\n",
    "        self.mazesdf['Choices_Presentation']=self.mazesdf['ChoicesCategory'].map(lambda x: self.encode_choices(x=x))\n",
    "        self.mazesdf['Step_Encoded']=self.mazesdf['NextFPmap'].map(lambda x: self.encode_next_step(x=x))\n",
    "\n",
    "    def construct_numpy_data(self, number_of_problems, return_maze_identifiers = False, np_shuffle_data = False):\n",
    "        # Create a new column which hold the vector for each problem\n",
    "        self.mazesdf['Problem_Vec']=self.mazesdf.apply(lambda x: self.create_problem_observation(row= x,goal_presentation_steps= self.goal_presentation_steps,delay_steps= self.delay_steps,choices_presentation_steps= self.choices_presentation_steps), axis=1)\n",
    "        # Set a random order of maze problems for the current session\n",
    "        self.mazes_order = np.random.randint(0,8,number_of_problems)\n",
    "\n",
    "        # Create vectors, holding observations and labels\n",
    "        session_observation =np.array([])\n",
    "        session_labels = np.array([])\n",
    "        for i in self.mazes_order:\n",
    "            session_observation = np.append(session_observation,self.mazesdf.iloc[i]['Problem_Vec'])\n",
    "            session_labels = np.append(session_labels,self.mazesdf.iloc[i]['Step_Encoded'])\n",
    "\n",
    "        # Reshape vectors to fit network observation and response space\n",
    "        session_length = self.goal_presentation_steps + self.delay_steps + self.choices_presentation_steps\n",
    "        session_observation = np.reshape(session_observation, (-1,session_length,8)).astype('float32')\n",
    "        session_labels = np.reshape(session_labels, (-1,4)).astype('float32')\n",
    "\n",
    "        # If np_shuffle_data == 'Labels, the order of labels is shuffled to randomise correct answers\n",
    "        if np_shuffle_data == 'Labels':\n",
    "          shuffle_generator = np.random.default_rng(38446)\n",
    "          shuffle_generator.shuffle(session_labels,axis=0)\n",
    "\n",
    "        # If return_maze_identifiers == 'IDs', return the array with maze IDs alongside the regular returns (observations, labels)\n",
    "        if return_maze_identifiers == 'IDs':\n",
    "          return session_observation, session_labels, self.mazes_order\n",
    "\n",
    "        return session_observation, session_labels\n",
    "\n",
    "    def construct_tf_data(self, number_of_problems, batch_size, tf_shuffle_data = False):\n",
    "        # Create dataset as described by numpy dataset function and transform it into a TF dataset\n",
    "        npds, np_labels = self.construct_numpy_data(number_of_problems=number_of_problems, np_shuffle_data = tf_shuffle_data)\n",
    "        tfdf = tf.data.Dataset.from_tensor_slices((npds, np_labels))\n",
    "        tfdf = tfdf.batch(batch_size)\n",
    "        return tfdf\n",
    "\n",
    "    def reset_construction_params(self, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
    "        self.goal_presentation_steps = goal_presentation_steps\n",
    "        self.delay_steps = delay_steps\n",
    "        self.choices_presentation_steps = choices_presentation_steps\n",
    "\n",
    "    def import_maze_dic(self, mazeDic=None):\n",
    "        if mazeDic == None:\n",
    "            # Set up dataframe with first choices of maze task\n",
    "            ## The dictionary was generated using MazeMetadata.py (v1.0.0) and the following call:\n",
    "            ### mazes.loc[(mazes['Nsteps']==2)&(mazes['ChoiceNo']=='ChoiceI')][['goal','ChoicesCategory','NextFPmap']].reset_index(drop=True).to_dict()\n",
    "            self.mazesDic = {'goal': {0: 9, 1: 9, 2: 19, 3: 17, 4: 17, 5: 7, 6: 19, 7: 7},\n",
    "            'ChoicesCategory': {0: 'ul',\n",
    "            1: 'rd',\n",
    "            2: 'ld',\n",
    "            3: 'rd',\n",
    "            4: 'ul',\n",
    "            5: 'ur',\n",
    "            6: 'lr',\n",
    "            7: 'lr'},\n",
    "            'NextFPmap': {0: 'u', 1: 'r', 2: 'd', 3: 'd', 4: 'l', 5: 'u', 6: 'r', 7: 'l'}}\n",
    "        else:\n",
    "            self.mazesDic = mazesDic\n",
    "        \n",
    "        # Create and return dataframe\n",
    "        return pd.DataFrame(self.mazesDic)\n",
    "\n",
    "    def encode_choices(self, x):\n",
    "        # Helper function to create the observation vector for choice periods\n",
    "        choices_sec = np.repeat(0,4)\n",
    "        choicesEncoding = pd.Series(list(x))\n",
    "        choicesEncoding = choicesEncoding.map({'l':1,'u':2,'r':3,'d':4})\n",
    "        for encodedChoice in choicesEncoding:\n",
    "            choices_sec[encodedChoice-1]=1\n",
    "        return np.concatenate((np.repeat(0,4),choices_sec))\n",
    "\n",
    "    def encode_next_step(self, x):\n",
    "        # Helper function to change the response / action to a OneHot encoded vector\n",
    "        step_sec = np.repeat(0,4)\n",
    "        stepEncoding = pd.Series(list(x))\n",
    "        stepEncoding = stepEncoding.map({'l':1,'u':2,'r':3,'d':4})\n",
    "        for encodedStep in stepEncoding:\n",
    "            step_sec[encodedStep-1]=1\n",
    "        return step_sec\n",
    "\n",
    "    def create_problem_observation(self, row, goal_presentation_steps, delay_steps, choices_presentation_steps):\n",
    "        # Helper function to create one vector describing the entire outline of one maze problem (Goal presentation, Delay Period, and Choices Presentation)\n",
    "        goal_vec = np.tile(row['Goal_Presentation'], goal_presentation_steps)\n",
    "        delay_vec = np.tile(np.repeat(0,8), delay_steps)\n",
    "        choices_vec = np.tile(row['Choices_Presentation'], choices_presentation_steps)\n",
    "        problem_vec = np.concatenate((goal_vec,delay_vec,choices_vec))\n",
    "        return problem_vec\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Maze DataSet Generator',\n",
    "            f'Goal Presentation Steps: {self.goal_presentation_steps}',\n",
    "            f'Delay Steps: {self.delay_steps}',\n",
    "            f'Choices Presentation Steps: {self.choices_presentation_steps}'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E_IiQ10JRvJ"
   },
   "source": [
    "### Generate datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT-3CUZCJVdI",
    "outputId": "4109601f-f81d-4a3e-f174-2c36bf132426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 50, 8), (None, 4)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# This constructor might run for around a minute to generate the dataset\n",
    "gen = mazeGeneratorI(goal_presentation_steps=20,delay_steps=10,choices_presentation_steps=20)\n",
    "tfdf = gen.construct_tf_data(number_of_problems=5120, batch_size=128)\n",
    "tfdf_test = gen.construct_tf_data(number_of_problems=2560, batch_size=128)\n",
    "tfdf_val = gen.construct_tf_data(number_of_problems=2560, batch_size=128)\n",
    "print(tfdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v6ZYJUbmJn-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=24, shape=(128, 50, 8), dtype=float32, numpy=\n",
      "array([[[0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.],\n",
      "        [0., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 1.],\n",
      "        [0., 0., 0., ..., 0., 0., 1.],\n",
      "        [0., 0., 0., ..., 0., 0., 1.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 1., 1., 0.],\n",
      "        [0., 0., 0., ..., 1., 1., 0.],\n",
      "        [0., 0., 0., ..., 1., 1., 0.]]], dtype=float32)>, <tf.Tensor: id=25, shape=(128, 4), dtype=float32, numpy=\n",
      "array([[0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Show example of dataset\n",
    "example_data = next(iter(tfdf))\n",
    "print(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoJwljfVKBpS"
   },
   "source": [
    "## seRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3fxROEHKGVa"
   },
   "source": [
    "### Regulariser\n",
    "\n",
    "In this section we define two regularisation functions:\n",
    "1. Regulariser for Euclidean embedding\n",
    "2. Subfunction which adds the communicability value (this is the one we use in seRNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6jsCMjDWK6uF"
   },
   "outputs": [],
   "source": [
    "class SE1(Regularizer):\n",
    "  \"\"\"A regulariser for sptially embedded RNNs.\n",
    "  Applies L1 regularisation to recurrent kernel of\n",
    "  RNN which is weighted by the distance of units\n",
    "  in predefined 3D space.\n",
    "  Calculation:\n",
    "      se1 * sum[distance_matrix o recurrent_kernel]\n",
    "  Attributes:\n",
    "      se1: Float; Weighting of SE1 regularisation term.\n",
    "      distance_tensor: TF tensor / matrix with cost per\n",
    "      connection in weight matrix of network.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, se1=0.01, neuron_num = 100, network_structure = (5,5,4), coordinates_list = None, distance_power = 1, distance_metric = 'euclidean'):  \n",
    "    self.version = 'v1.1.0'\n",
    "    self.distance_power = distance_power\n",
    "    \n",
    "    # Set SE1 regularisation strength to default of 0.01 if no value given\n",
    "    se1 = 0.01 if se1 is None else se1\n",
    "    self._check_penalty_number(se1)\n",
    "\n",
    "    # Transform regularisation strength to TF's standard float format \n",
    "    self.se1 = backend.cast_to_floatx(se1)\n",
    "\n",
    "    # Set up tensor with distance matrix\n",
    "    ## Set up neurons per dimension\n",
    "    nx = np.arange(network_structure[0])\n",
    "    ny = np.arange(network_structure[1])\n",
    "    nz = np.arange(network_structure[2])\n",
    "\n",
    "    ## Set up coordinate grid\n",
    "    [x,y,z] = np.meshgrid(nx,ny,nz)\n",
    "    self.coordinates = [x.ravel(),y.ravel(),z.ravel()]\n",
    "\n",
    "    ## Override coordinate grid if one if provided in init\n",
    "    if coordinates_list!=None:\n",
    "      self.coordinates = coordinates_list\n",
    "\n",
    "    ## Check neuron number / number of coordinates\n",
    "    if (len(self.coordinates[0])==neuron_num)&(len(self.coordinates[1])==neuron_num)&(len(self.coordinates[2])==neuron_num):\n",
    "      pass\n",
    "    else:\n",
    "      raise ValueError('Network / coordinate structure does not match the number of neurons.')\n",
    "\n",
    "    ## Calculate the euclidean distance matrix\n",
    "    euclidean_vector = scipy.spatial.distance.pdist(np.transpose(self.coordinates), metric=distance_metric)\n",
    "    euclidean = scipy.spatial.distance.squareform(euclidean_vector**self.distance_power)\n",
    "    self.distance_matrix = euclidean.astype('float32')\n",
    "    self.spatial_cost_matrix = self.distance_matrix\n",
    "\n",
    "    ## Add minimal cost for recurrent self connection (on diagonal)\n",
    "    #diag_dist = np.diag(np.repeat(0.1,100)).astype('float32')\n",
    "    #self.distance_matrix = self.distance_matrix + diag_dist\n",
    "\n",
    "    ## Create tensor from distance matrix\n",
    "    self.distance_tensor =  tf.convert_to_tensor(self.distance_matrix)\n",
    "\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # Add calculation of loss here.\n",
    "    # L1 for reference: self.l1 * math_ops.reduce_sum(math_ops.abs(x))\n",
    "    abs_weight_matrix = tf.math.abs(x)\n",
    "\n",
    "    #se1_loss = self.se1 * tf.math.multiply(abs_weight_matrix, self.distance_tensor)\n",
    "    #se1_loss = tf.math.reduce_sum(abs_weight_matrix)\n",
    "    se1_loss = self.se1 * tf.math.reduce_sum(tf.math.multiply(abs_weight_matrix, self.distance_tensor))\n",
    "    \n",
    "    return se1_loss\n",
    "\n",
    "  def _check_penalty_number(self, x):\n",
    "    \"\"\"check penalty number availability, raise ValueError if failed.\"\"\"\n",
    "    if not isinstance(x, (float, int)):\n",
    "      raise ValueError(('Value: {} is not a valid regularization penalty number, '\n",
    "                        'expected an int or float value').format(x))\n",
    "\n",
    "  def visualise_distance_matrix(self):\n",
    "    plt.imshow(self.distance_matrix)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "  def visualise_neuron_structure(self):\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter(self.coordinates[0],self.coordinates[1],self.coordinates[2],c='b',marker='.')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    plt.show()\n",
    "\n",
    "  def get_config(self):\n",
    "    return {'se1': float(self.se1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hLVlbpnkK_i2"
   },
   "outputs": [],
   "source": [
    "class SE1_sWc(SE1):\n",
    "    '''\n",
    "    Version of SE1 regulariser which combines the spatial and communicability parts in loss function.\n",
    "    Additional comms_factor scales the communicability matrix.\n",
    "    The communicability term used here is unbiased weighted communicability:\n",
    "    Crofts, J. J., & Higham, D. J. (2009). A weighted communicability measure applied to complex brain networks. Journal of the Royal Society Interface, 6(33), 411-414.\n",
    "    '''\n",
    "    def __init__(self, se1=0.01, comms_factor = 1, neuron_num = 100, network_structure = (5,5,4), coordinates_list = None, distance_power = 1, distance_metric = 'euclidean'):\n",
    "      SE1.__init__(self, se1, neuron_num , network_structure , coordinates_list, distance_power , distance_metric)\n",
    "      self.comms_factor = comms_factor\n",
    "\n",
    "    def __call__(self, x):\n",
    "      # Take absolute of weights\n",
    "      abs_weight_matrix = tf.math.abs(x)\n",
    "\n",
    "      # Calulcate weighted communicability (see reference in docstring)\n",
    "      stepI = tf.math.reduce_sum(abs_weight_matrix, axis=1)\n",
    "      stepII = tf.math.pow(stepI, -0.5)\n",
    "      stepIII = tf.linalg.diag(stepII)\n",
    "      stepIV = tf.linalg.expm(stepIII@abs_weight_matrix@stepIII)\n",
    "      comms_matrix = tf.linalg.set_diag(stepIV, tf.zeros(stepIV.shape[0:-1]))\n",
    "\n",
    "      # Multiply absolute weights with communicability weights\n",
    "      comms_matrix = comms_matrix**self.comms_factor\n",
    "      comms_weight_matrix = tf.math.multiply(abs_weight_matrix, comms_matrix)\n",
    "\n",
    "      # Multiply comms weights matrix with distance tensor, scale the mean, and return as loss\n",
    "      se1_loss = self.se1 * tf.math.reduce_sum(tf.math.multiply(comms_weight_matrix , self.distance_tensor))\n",
    "      \n",
    "      return se1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtsBAdHCQG12"
   },
   "source": [
    "### Model training helper functions\n",
    "\n",
    "Here we define a callback to give us easy access to weight matrices after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tNx_GNBcQSzb"
   },
   "outputs": [],
   "source": [
    "class RNNWeightMatrixHistoryI(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Saves the RNN_Weight_Matrix to the training history before\n",
    "    the start of training and after finishing each epoch.\n",
    "\n",
    "    The network model needs to be build manually before calling fit() method\n",
    "    for this callback to work.\n",
    "    '''\n",
    "    def __init__(self, RNN_layer_number = 0):\n",
    "        super(RNNWeightMatrixHistoryI, self).__init__()\n",
    "        self.RNN_layer_number = RNN_layer_number\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Create key for RNN_Weight_Matrix in history\n",
    "        self.model.history.history['RNN_Weight_Matrix'] = []\n",
    "        #print(\"Created key for RNN_Weight_Matrix in history.\")\n",
    "\n",
    "        # Save matrix before start of training\n",
    "        self.model.history.history['RNN_Weight_Matrix'].append(self.model.layers[self.RNN_layer_number].get_weights()[1])\n",
    "        #print(\"Saved RNN_Weight_Matrix to history.\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Save RNN_Weight_Matrix to history\n",
    "        self.model.history.history['RNN_Weight_Matrix'].append(self.model.layers[self.RNN_layer_number].get_weights()[1])\n",
    "        #print(\"Saved RNN_Weight_Matrix to history.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aQQSb6SKMLK"
   },
   "source": [
    "### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JC04f--UKOat",
    "outputId": "e4a1d78f-4572-4c54-983d-ba2365e75cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3500006499999999\n"
     ]
    }
   ],
   "source": [
    "regu_space_full = np.linspace(0.000001,1,1001)\n",
    "regu_strength = regu_space_full[350]\n",
    "print(regu_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FMS3FkPQKWOR"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (simple_rnn/strided_slice:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-855bd4fd845c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    671\u001b[0m                            'method accepts an `inputs` argument.')\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_recurrent_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m     return super(SimpleRNN, self).call(\n\u001b[0;32m-> 1393\u001b[0;31m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    680\u001b[0m            constants=None):\n\u001b[1;32m    681\u001b[0m     inputs, initial_state, constants = self._process_inputs(\n\u001b[0;32m--> 682\u001b[0;31m         inputs, initial_state, constants)\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       init_state = get_initial_state_fn(\n\u001b[0;32m--> 606\u001b[0;31m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m       init_state = _generate_zero_filled_state(batch_size, self.cell.state_size,\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state_for_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2750\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2752\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2770\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2763\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2765\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2347\u001b[0m         \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m         \u001b[0;31m# to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2304\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2306\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2307\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/weighted_generative_model/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (simple_rnn/strided_slice:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "regu = SE1_sWc(se1=regu_strength)\n",
    "coord = regu.coordinates\n",
    "cost = regu.spatial_cost_matrix\n",
    "\n",
    "## Assemble network\n",
    "tf_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GaussianNoise(stddev=0.05),\n",
    "    tf.keras.layers.SimpleRNN(100, activation='relu',recurrent_initializer='orthogonal', return_sequences=False, recurrent_regularizer= regu),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "tf_model.build(input_shape=example_data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q7MSHLzolA4"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltW_nLrgO8ff"
   },
   "outputs": [],
   "source": [
    "tf_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7k3pOnJ-PFUQ",
    "outputId": "26ba2672-97c2-42a6-e4d9-abe785c4a748"
   },
   "outputs": [],
   "source": [
    "history = tf_model.fit(tfdf, epochs=10,validation_data=tfdf_test,\n",
    "                       callbacks=RNNWeightMatrixHistoryI(RNN_layer_number=1)\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-OEkjfIookA"
   },
   "source": [
    "### Structural analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ftpn1KPsPs_b",
    "outputId": "917e7d13-cb4f-43a9-f576-62a2bb468d09"
   },
   "outputs": [],
   "source": [
    "#pip install bctpy\n",
    "import bct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpL1R-8XRHDF"
   },
   "outputs": [],
   "source": [
    "# Extract example absolute weight matrix from trained network\n",
    "history_dic = history.history\n",
    "example_weight_matrix = np.abs(history_dic['RNN_Weight_Matrix'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvcX8Uy1S1IK"
   },
   "outputs": [],
   "source": [
    "# Binarise network before structural analysis\n",
    "binary_weight_matrix = example_weight_matrix.copy()\n",
    "thresh = np.quantile(example_weight_matrix, q=0.9)\n",
    "matrix_mask = example_weight_matrix > thresh\n",
    "binary_weight_matrix[matrix_mask] = 1\n",
    "binary_weight_matrix[~matrix_mask] = 0\n",
    "#binary_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ck6_y-dtWmAy",
    "outputId": "3f7da694-8a52-4bbf-c520-b5680398872d"
   },
   "outputs": [],
   "source": [
    "# Calculate modularity q-statistic\n",
    "## Note that the example network chose here shows relatively high modularity value\n",
    "_, q_stat = bct.modularity_und(binary_weight_matrix, gamma=1)\n",
    "print(q_stat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
